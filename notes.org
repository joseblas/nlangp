; -*- mode: org;-*-

* a1

Create a trigram hidden Markov model to do named entity recognition of gene names in a document about genetics.

** part 1

#+begin_example
$ make a1_p1_all
python hmm.py p1 > data/gene_dev.p1.out
python eval_gene_tagger.py data/gene.key data/gene_dev.p1.out
Found 2616 GENEs. Expected 642 GENEs; Correct: 418.

	 precision 	recall 		F1-Score
GENE:	 0.159786	0.651090	0.256599
python hmm.py p1 --gene-data gene.test > data/gene_test.p1.out
python hmm.py p1 --emission df_em_wo_rare > data/gene_dev.df_em_wo_rare.p1.out
python eval_gene_tagger.py data/gene.key data/gene_dev.df_em_wo_rare.p1.out
Found 2616 GENEs. Expected 642 GENEs; Correct: 418.

	 precision 	recall 		F1-Score
GENE:	 0.159786	0.651090	0.256599
python hmm.py p1 --emission df_em_kept_rare > data/gene_dev.df_em_kept_rare.p1.out
python eval_gene_tagger.py data/gene.key data/gene_dev.df_em_kept_rare.p1.out
Found 2236 GENEs. Expected 642 GENEs; Correct: 424.

	 precision 	recall 		F1-Score
GENE:	 0.189624	0.660436	0.294649
#+end_example
15% improvement to keep rare words.

** part 2

in =e(word|tag)=, if upon not finding =(tag,word)=, replace word with
"_RARE_":
#+BEGIN_EXAMPLE
python eval_gene_tagger.py data/gene.key data/gene_dev.p2.out
Found 2718 GENEs. Expected 642 GENEs; Correct: 42.

	 precision 	recall 		F1-Score
GENE:	 0.015453	0.065421	0.025000
#+END_EXAMPLE
the problem is that, if word is under another tag, it is not rare, so
failure to find (tag,word) should emit 0, rather than replacing the word
with =_RARE_= and looking for =(tag,'_RARE_')=, which will overpredict
I-GENE (and not find many either).

In other words, the problem is a conceptual flaw. One only uses =_RARE_=
if the word is nowhere in the corpus, not just not emitted by this tag,
because it can be emitted by some other tag.

So in =e(word|tag)=, if upon not finding =(tag,word)=, check if word is
emitted by another other tag, returning 0 if so, but if not, replace
word with =_RARE_=:
#+begin_example
python eval_gene_tagger.py data/gene.key data/gene_dev.p2.out
Found 388 GENEs. Expected 642 GENEs; Correct: 203.

	 precision 	recall 		F1-Score
GENE:	 0.523196	0.316199	0.394175
#+end_example

*** what ifs

if using df_em_wo_rare, and so setting e() to 0, either because
(word=_RARE_,tag) will not be found, or the word is under another tag:
#+BEGIN_EXAMPLE
python eval_gene_tagger.py data/gene.key data/gene_dev.p2.out
Found 72 GENEs. Expected 642 GENEs; Correct: 42.

	 precision 	recall 		F1-Score
GENE:	 0.583333	0.065421	0.117647
#+END_EXAMPLE



if using df_em_kept_rare (ie. keeping all rare words)
#+begin_example
python eval_gene_tagger.py data/gene.key data/gene_dev.p2.out
Found 471 GENEs. Expected 642 GENEs; Correct: 253.

	 precision 	recall 		F1-Score
GENE:	 0.537155	0.394081	0.454627
#+end_example
a 15% improvement, similar to the unigram case.  The same performance
might be kept, with memory requirements mitigated, using a bloom filter.
** part 3

#+begin_example
$ make a1_p3_eval
python hmm.py p3 > data/gene_dev.p3.out
python eval_gene_tagger.py data/gene.key data/gene_dev.p3.out
Found 443 GENEs. Expected 642 GENEs; Correct: 229.

	 precision 	recall 		F1-Score
GENE:	 0.516930	0.356698	0.422120
#+end_example
7% improvement
** future

- linear interpolation for q()
* a2

Create a PCFG for syntactic parsing of questions.

#+begin_quote
Expected development total F1-Scores are 0.79 for part 2 and 0.83 for part 3.
(from assignment page)
#+end_quote

** ideas

- effect of using \arg\max \pi[1,n,X] instead of \pi[1,n,SBARQ].

** qs

why does backtrace() only retrieve one? because it is a dict().

it stores the split point declared "best" (under a given (i,j,X)), so
starting with (1,n,SBARQ), the split point could be anywhere between
[1,n]. if it is a brule, split and recurse. within one subtree, find the
split between [i,s], and so on, until arriving at a leaf, where y2 would
be undefined, and the word should be yielded.

why would it not yield everything? Because 
[i,s,y1] or [s+1,j,y2] fixes 
X in [i,i,X], which is a key, and thus there is only 1 value assigned
(even if _RARE_)

** runtimes
** eval
*** p2
current:
#+begin_example
$ time make a2_p2_eval
      Type       Total   Precision      Recall     F1-Score
===============================================================
      ADJP          13     0.375        0.231        0.286
      ADVP          20     0.400        0.100        0.160
        NP        1081     0.691        0.722        0.706
        PP         326     0.724        0.755        0.739
       PRT           6     1.000        0.167        0.286
        QP           2     0.000        0.000        0.000
         S          45     0.438        0.156        0.230
      SBAR          15     0.333        0.133        0.190
     SBARQ         488     0.972        0.998        0.985
        SQ         488     0.880        0.902        0.891
        VP         305     0.529        0.331        0.407
    WHADJP          43     0.796        0.907        0.848
    WHADVP         125     0.960        0.968        0.964
      WHNP         372     0.875        0.866        0.870
      WHPP          10     1.000        0.600        0.750

     total        3339     0.788        0.766        0.777

real	19m45.803s
user	18m3.408s
sys	0m8.661s
#+end_example

*** p3

#+BEGIN_EXAMPLE
$ time make a2_p3_eval
      Type       Total   Precision      Recall     F1-Score
===============================================================
      ADJP          13     0.286        0.154        0.200
      ADVP          20     0.143        0.050        0.074
        NP        1081     0.688        0.725        0.706
        PP         326     0.680        0.684        0.682
       PRT           6     1.000        0.333        0.500
        QP           2     0.000        0.000        0.000
         S          45     0.519        0.311        0.389
      SBAR          15     0.385        0.333        0.357
     SBARQ         488     0.976        0.998        0.987
        SQ         488     0.952        0.969        0.960
        VP         305     0.537        0.430        0.477
    WHADJP          43     0.884        0.884        0.884
    WHADVP         125     0.946        0.976        0.961
      WHNP         372     0.964        0.925        0.944
      WHPP          10     1.000        0.500        0.667

     total        3339     0.798        0.788        0.793

real    26m27.723s
user    25m50.831s
sys     0m30.574s
#+END_EXAMPLE
slightly lower than the course expected value, but higher than non-markovization.
* a3

#+begin_quote
The expected development F-Scores are 0.420, 0.449, and a basic intersection alignment should give 0.485 for the last part.
#+end_quote
